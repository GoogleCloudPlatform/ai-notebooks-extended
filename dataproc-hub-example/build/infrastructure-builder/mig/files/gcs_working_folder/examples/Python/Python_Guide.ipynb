{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "1. [Google Cloud Storage](http://35.222.229.5:8080/lab?#Google-Cloud-Storage)\n",
    "2. [BigQuery](http://35.222.229.5:8080/lab?#BigQuery)\n",
    "3. [Cloud SQL](http://35.222.229.5:8080/lab?#Cloud-SQL)\n",
    "4. [Installing Python modules using pip](http://35.222.229.5:8080/lab?#Installing-Python-modules-using-pip)\n",
    "5. [Authenticating Python API clients for GCS, Bigquery and Cloud SQL](http://35.222.229.5:8080/lab?#Authenticating-Python-API-clients-for-GCS,-Bigquery-and-Cloud-SQL)\n",
    "6. [Creating Pandas Dataframes](http://35.222.229.5:8080/lab?#Creating-Pandas-Dataframes)\n",
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage\n",
    "Refer to the storage directory in the same folder.\n",
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery\n",
    "Refer to the bigquery directory in the same folder.\n",
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud SQL\n",
    "Refer to the sql directory in the same folder\n",
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Python modules using pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is the command to install package pymysql from the command line:\n",
    "\n",
    "    `pip install pymysql`\n",
    "\n",
    "- To run shell commands from inside the JupyterLab notebooks, we need to append the command with an exclamation mark:\n",
    "\n",
    "    `!pip install pymysql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JupyterLab notebook VMs created using the initialization script will come pre-installed with a wide number of python packages. Please refer\n",
    "the TDD document Section [3.1.2](https://docs.google.com/document/d/1XhoEKoVig7puLtHc-2uG3gjnlcRb6I3r7i7cOikRB5k/edit#heading=h.uovjuz1q44ec) to review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating Python API clients for GCS, Bigquery and Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Service Account JSON key for authentication\n",
    "An authentication call to the GCS, BQ and Cloud SQL can be made by using a valid Service Account JSON key. \\\n",
    "If you do not have a Service Account yet, follow the steps given [here](https://cloud.google.com/iam/docs/creating-managing-service-accounts) to retrieve Service Account JSON key. \\\n",
    "Service Account JSON key retreival involves below steps: \\\n",
    "    1. Create a Service Account \\\n",
    "    2. Assign it required IAM permissions \\\n",
    "    3. Download Service Account JSON key \\\n",
    "    4. Upload the key from where it can be used \\\n",
    "    \n",
    "Once the key has been obtained, code snippets provided below can be used to authenticate the user and connect to the services via Python APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Storage\n",
    "\n",
    "Provide the key path to the JSON key for valid Service Account in order to validate a GCS client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"name-of-your-bucket\"\n",
    "file_name = \"filename.extension\"\n",
    "key_path = \"path/to/service_account.json\"\n",
    "\n",
    "# TODO(developer): \n",
    "# Set key_path to the path to the service account key file.\n",
    "# key_path = \"path/to/service_account.json\"\n",
    "# Set the bucket_name to the name of the bucket you want to read from\n",
    "# Set the file_name to the file you want to read along with folder structure inside the mentioned bucket\n",
    "# bucket_name and file_name variables are used to create the GCS object URL: gs://bucket_name/file_name\n",
    "\n",
    "storage_client = storage.Client.from_service_account_json(key_path)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(file_name)\n",
    "#Below command will download all the contents of the specified GCS object and save it in string format\n",
    "contents = blob.download_as_string()\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery\n",
    "\n",
    "Provide the key path to the JSON key for valid Service Account in order to validate a BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas\n",
    "\n",
    "# TODO(developer): \n",
    "# Set key_path to the path to the service account key file.\n",
    "# key_path = \"path/to/service_account.json\"\n",
    "\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(\n",
    "    credentials=credentials,\n",
    "    project=credentials.project_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud SQL\n",
    "\n",
    "The Cloud SQL proxy `connection` can be authenticated at the time of connection creation. Please refer [this](https://cloud.google.com/sql/docs/mysql/connect-external-app) guide to learn more about Cloud SQL authentication.\n",
    "\n",
    "Download the [Cloud SQL proxy](https://cloud.google.com/sql/docs/mysql/sql-proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy\n",
    "!chmod +x cloud_sql_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below command from the terminal to start SQLproxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(developer): \n",
    "# Set PATH_TO_KEY_FILE to the path to the service account key file.\n",
    "# Example: PATH_TO_KEY_FILE = \"path/to/service_account.json\"\n",
    "\n",
    "!./cloud_sql_proxy -instances=<INSTANCE_NAME>=tcp:3306 -credential_file=<PATH_TO_KEY_FILE> &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# TODO(developer): \n",
    "# Change USERNAME and PASSWORD to the user and password created on Cloud SQL instance\n",
    "# Set DB to the name of the database to be connected to\n",
    "\n",
    "connection = pymysql.connect(host='127.0.0.1',\n",
    "                             user='USERNAME',\n",
    "                             password='PASSWORD',\n",
    "                             db='DB')\n",
    "mycursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pandas Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery\n",
    "\n",
    "Use the [Client.query](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.query) method to run the query, and the [QueryJob.to_dataframe](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.QueryJob.html#google.cloud.bigquery.job.QueryJob.to_dataframe) method to return the results as a pandas [`DataFrame`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "query = \"\"\"\n",
    "    SELECT name, SUM(number) as total\n",
    "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "    GROUP BY name\n",
    "    ORDER BY total DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Storage\n",
    "\n",
    "Panda's [read_csv](https://pandas.pydata.org/pandas-docs/version/0.18/generated/pandas.read_csv.html) method supports reading dataframes directly from GCS file URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('gs://BUCKET/your_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud SQL\n",
    "\n",
    "Panda's [read_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html) method can be used to read a table from Cloud SQL database and use it as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# TODO(developer): \n",
    "# Change USERNAME and PASSWORD to the user and password created on Cloud SQL instance\n",
    "# Set DB to the name of the database to be connected to\n",
    "\n",
    "connection = pymysql.connect(host='127.0.0.1',\n",
    "                             user='USERNAME',\n",
    "                             password='PASSWORD',\n",
    "                             db='DB')\n",
    "query = \"SELECT * FROM orders WHERE date_time BETWEEN ? AND ?\"\n",
    "df = pd.read_sql(query, connection,  params=(start_date, end_date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
