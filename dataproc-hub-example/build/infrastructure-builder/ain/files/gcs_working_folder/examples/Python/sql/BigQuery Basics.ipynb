{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot run multiple SparkContexts at once; existing SparkContext(app=livy-session-4, master=yarn) created by __init__ at /hadoop/yarn/nm-local-dir/usercache/livy/appcache/application_1565144474353_0029/container_1565144474353_0029_01_000001/tmp/5188923193758582290:595 \n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/livy/appcache/application_1565144474353_0029/container_1565144474353_0029_01_000001/pyspark.zip/pyspark/context.py\", line 129, in __init__\n",
      "    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/livy/appcache/application_1565144474353_0029/container_1565144474353_0029_01_000001/pyspark.zip/pyspark/context.py\", line 328, in _ensure_initialized\n",
      "    callsite.function, callsite.file, callsite.linenum))\n",
      "ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=livy-session-4, master=yarn) created by __init__ at /hadoop/yarn/nm-local-dir/usercache/livy/appcache/application_1565144474353_0029/container_1565144474353_0029_01_000001/tmp/5188923193758582290:595 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the InputFormat. This assumes the Cloud Storage connector for\n",
    "# Hadoop is configured.\n",
    "bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
    "\n",
    "conf = {\n",
    "    # Input Parameters.\n",
    "    'mapred.bq.project.id': project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': input_directory,\n",
    "    'mapred.bq.input.project.id': 'publicdata',\n",
    "    'mapred.bq.input.dataset.id': 'samples',\n",
    "    'mapred.bq.input.table.id': 'shakespeare',\n",
    "}\n",
    "\n",
    "# Output Parameters.\n",
    "output_dataset = 'wordcount_dataset'\n",
    "output_table = 'wordcount_output'\n",
    "\n",
    "# Load data in from BigQuery.\n",
    "table_data = sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=conf)\n",
    "\n",
    "# Perform word count.\n",
    "word_counts = (\n",
    "    table_data\n",
    "    .map(lambda record: json.loads(record[1]))\n",
    "    .map(lambda x: (x['word'].lower(), int(x['word_count'])))\n",
    "    .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "# Display 10 results.\n",
    "pprint.pprint(word_counts.take(10))\n",
    "\n",
    "# Stage data formatted as newline-delimited JSON in Cloud Storage.\n",
    "output_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n",
    "output_files = output_directory + '/part-*'\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "(word_counts\n",
    " .toDF(['word', 'word_count'])\n",
    " .write.format('json').save(output_directory))\n",
    "\n",
    "# Shell out to bq CLI to perform BigQuery import.\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files}'.format(\n",
    "        dataset=output_dataset, table=output_table, files=output_files\n",
    "    ).split())\n",
    "\n",
    "# Manually clean up the staging_directories, otherwise BigQuery\n",
    "# files will remain indefinitely.\n",
    "input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
    "input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True)\n",
    "output_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\n",
    "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "    output_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
