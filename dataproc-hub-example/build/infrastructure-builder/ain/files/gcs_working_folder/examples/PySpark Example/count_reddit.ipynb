{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Example to access BigQuery and GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is supposed to work as a tutorial to submit Spark jobs to access BigQuery and GCS resources using JupyterLab running on a remote VM. In this example we will be reading data from a open source bigquery table `2016_01` available in `reddit-posts` database of `fh-bigquery` project. As an output we generate the total number of comments made in each subreddit and then print the top 20 subreddits with most comments as result.\n",
    "\n",
    "## BigQuery Spark connector\n",
    "\n",
    "Before running the below code, it is essential to have [BigQuery-Spark connector](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example) propoerly positioned on the Dataproc master node. When Livy REST service is submitting a Spark job, it needs to provide this spark-bigquery connector to Spark master node, in order for Dataproc to access BigQuery table. \n",
    "\n",
    "The connector can be downloaded using below command, it is hosted on a public bucket by Google. This bucket needs to be saved to `livy_installation_path/repl*` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-latest.jar ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCS Spark Connector\n",
    "\n",
    "All Dataproc clusters come pre-installed with [GCS connectors](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters). This means that Spark cluster itself can reach GCS Spark connector jar files and hence do not require manual installation.\n",
    "\n",
    "## Flow of Operation\n",
    "\n",
    "When the below cell is executed:\n",
    "\n",
    "1. [Sparkmagic](https://github.com/jupyter-incubator/sparkmagic) will pick up code from the JupyterLab cell and send it over to Livy REST server\n",
    "2. [Apache Livy](https://livy.incubator.apache.org) REST service, which is listening on port 8998 of Dataproc Master node, will accept the spark job request and based on the configs made in livy.conf file, it will submit a Spark job to the master node.\n",
    "3. Master node will execute the spark job, on the cluster according to config, and return result back to Livy REST service.\n",
    "4. Livy REST service send the result back to JupyterLab VM via Sparkmagic and result of the operation gets displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following table will be accounted for in our analysis:\n",
      "fh-bigquery.reddit_posts.2016_01\n",
      "+--------------------+------+\n",
      "|           subreddit| count|\n",
      "+--------------------+------+\n",
      "|           AskReddit|187508|\n",
      "|GlobalOffensiveTrade|174022|\n",
      "|           Fireteams|102953|\n",
      "|     leagueoflegends| 68740|\n",
      "|               funny| 66569|\n",
      "|                news| 58487|\n",
      "|              videos| 57241|\n",
      "|      Showerthoughts| 46319|\n",
      "|                pics| 39492|\n",
      "|     GlobalOffensive| 38365|\n",
      "|              gaming| 37060|\n",
      "|        dirtykikpals| 32270|\n",
      "|                 aww| 30697|\n",
      "|          betternews| 29856|\n",
      "|           worldnews| 28527|\n",
      "|        dirtypenpals| 27669|\n",
      "|        pcmasterrace| 25965|\n",
      "|               Music| 25188|\n",
      "|                spam| 23770|\n",
      "|              movies| 23683|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "# A Spark Session is how we interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This will help catch some PySpark errors\n",
    "# from py4j.protocol import Py4JJavaError\n",
    "\n",
    "# Create a SparkSession under the name \"reddit\". Viewable via the Spark UI\n",
    "spark = SparkSession.builder.appName(\"reddit\").getOrCreate()\n",
    "\n",
    "# Create a two column schema consisting of a string and a long integer\n",
    "fields = [StructField(\"subreddit\", StringType(), True),\n",
    "          StructField(\"count\", LongType(), True)]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Create an empty DataFrame. We will continuously union our output with this\n",
    "subreddit_counts = spark.createDataFrame([], schema)\n",
    "\n",
    "# Keep track of all tables accessed via the job\n",
    "tables_read = []\n",
    "\n",
    "year = \"2016\"\n",
    "month = \"01\"\n",
    "# In the form of <project-id>.<dataset>.<table>\n",
    "table = \"fh-bigquery.reddit_posts.{0}_{1}\".format(year, month)\n",
    "\n",
    "# If the table doesn't exist we will simply continue and not\n",
    "# log it into our \"tables_read\" list\n",
    "\n",
    "table_df = spark.read.format('bigquery').option('table', table).load()\n",
    "tables_read.append(table)\n",
    "\n",
    "# We perform a group-by on subreddit, aggregating by the count and then\n",
    "# unioning the output to our base dataframe\n",
    "subreddit_counts = (\n",
    "    table_df\n",
    "    .groupBy(\"subreddit\")\n",
    "    .count()\n",
    "    .union(subreddit_counts)\n",
    ")\n",
    "        \n",
    "print(\"The following table will be accounted for in our analysis:\")\n",
    "print(table)\n",
    "\n",
    "# From our base table, we perform a group-by, summing over the counts.\n",
    "# We then rename the column and sort in descending order both for readability.\n",
    "result = (\n",
    "    subreddit_counts\n",
    "    .groupBy(\"subreddit\")\n",
    "    .sum(\"count\")\n",
    "    .withColumnRenamed(\"sum(count)\", \"count\")\n",
    "    .sort(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# show() will collect the table into memory output the table to std out.\n",
    "(\n",
    "result.show()\n",
    ")\n",
    "\n",
    "output_bucket_name = \"ankit-spark\"\n",
    "file_uri = \"my_output_folder/bq_output.csv\"\n",
    "# Save the result in the form of a CSV file\n",
    "# The command given below will save multiple CSV files because Spark will collect output from multiple processes \n",
    "# and save each of them in a separate CSV\n",
    "# result.write.csv(\"gs://{0}/{1}\".format(output_bucket_name, file_uri))\n",
    "\n",
    "# To get a single CSV file as output, use the coalesce(1) before passing the dataframe to GCS\n",
    "result.coalesce(1).write.csv(\"gs://{0}/{1}\".format(output_bucket_name, file_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
